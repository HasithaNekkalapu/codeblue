{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml_algortithms.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVd9oDHNqryU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6k6rWR5UqvZw",
        "colab_type": "text"
      },
      "source": [
        "# KMEANS\n",
        "\n",
        "k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.\n",
        "\n",
        "The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the **expectation-maximization algorithm for mixtures of Gaussian distributions** via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.\n",
        "\n",
        "The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.\n",
        "\n",
        "The term \"k-means\" was first used by James MacQueen in 1967,though the idea goes back to Hugo Steinhaus in 1956.The standard algorithm was first proposed by Stuart Lloyd of Bell Labs in 1957 as a technique for pulse-code modulation, though it wasn't published as a journal article until 1982. In 1965, Edward W. Forgy published essentially the same method, which is why it is sometimes referred to as Lloyd-Forgy.\n",
        "\n",
        "k-means clustering, and its associated expectation-maximization algorithm, is a special case of a **Gaussian mixture model**, specifically, the limit of taking all covariances as diagonal, equal and small. It is often easy to generalize a k-means problem into a Gaussian mixture model. Another generalization of the k-means algorithm is the **K-SVD algorithm**, which estimates data points as a sparse linear combination of \"codebook vectors\". k-means corresponds to the special case of using a single codebook vector, with a weight of 1.\n",
        "\n",
        "\n",
        "**Applications**\n",
        "* k-means clustering is rather easy to apply to even large data sets, particularly when using heuristics such as Lloyd's algorithm. It has been successfully used in market segmentation, computer vision, and astronomy among many other domains. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.\n",
        "\n",
        "* Vector quantization :\n",
        "k-means originates from signal processing, and still finds use in this domain. For example, in computer graphics, color quantization is the task of reducing the color palette of an image to a fixed number of colors k. The k-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is image segmentation. Other uses of vector quantization include non-random sampling, as k-means can easily be used to choose k different but prototypical objects from a large data set for further analysis.\n",
        "\n",
        "* Cluster analysis :\n",
        "\n",
        "In cluster analysis, the k-means algorithm can be used to partition the input data set into k partitions (clusters).\n",
        "\n",
        "**Advantages** \n",
        "*\n",
        "*\n",
        "*\n",
        "*\n",
        "**Disadvantages**\n",
        "* The pure k-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case). \n",
        "* In particular, the parameter **k is known to be hard to choose** when not given by external constraints. \n",
        "* Another limitation is that it **cannot be used with arbitrary distance** functions \n",
        "* **Cannot be used with non-numerical data.**\n",
        "\n",
        "### Refereces\n",
        "\n",
        "* https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a\n",
        "* https://en.wikipedia.org/wiki/K-means_clustering\n",
        "* \n"
      ]
    }
  ]
}